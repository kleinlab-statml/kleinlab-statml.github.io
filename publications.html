<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description" content="Homepage of KleinLab">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>KleinLab - Homepage</title>

  <link href="./libs/bootstrap-3.4.1/css/bootstrap.css" rel="stylesheet">
  <link href="./libs/fontawesome-6.2.0/css/all.min.css" rel="stylesheet">
  <link href="style.css" rel="stylesheet">


  <link href='https://fonts.googleapis.com/css?family=Didact Gothic' rel='stylesheet'>
  <style>
  body {
   font-family: 'Didact Gothic';
  }
  </style>

<link rel="icon" href="images/logo_light_wo_frame.png">

</head>
<body data-spy="scroll" data-target="#myScrollspy" data-offset="80">

	<a href="#" id="scrollbutton" class="hidden-md hidden-lg hidden-xl" style="display: none;"><span></span></a>

<div class="container">
	
<h2 style="color:#164194;">KleinLab - Uncertainty Quantification and Statistical Learning</h2>

  <nav class="navbar navbar-light">	  
	<ul class="nav navbar-nav mr-auto">
		<li class="nav-item"><a class="nav-link" href="index.html">Home</a></li>
		<li class="nav-item"><a class="nav-link"href="news.html">News</a></li>
		<li class="nav-item"><a class="nav-link" href="team.html">Team</a></li>
		<li class="nav-item"><a class="nav-link" href="research.html">Research</a></li>
		<li class="nav-item active"><a class="nav-link" href="publications.html">Publications</a></li>
		<li class="nav-item"><a class="nav-link" href="teaching.html">Teaching</a></li>
		<li class="nav-item"><a class="nav-link" href="jobs.html">Jobs</a></li>
		<li class="nav-item"><a class="nav-link" href="contact.html">Contact</a></li>
	</ul>
  </nav>

		
<!-- ------------------------------------------------------------------------------------------------------------------------------------ -->
<!-- ------------------------------------------------------------------------------------------------------------------------------------ -->
<!-- ------------------------------------------------------------------------------------------------------------------------------------ -->
<!-- ------------------------------------------------------------------------------------------------------------------------------------ -->

			<!-- Publications -->
			<section id="publications" style="font-size:medium;">
				<h2>Publications <span class="fa fa-book" aria-hidden="true" style="float:left;padding-right:15px;padding-top:6px;font-size: 75%;"></span></h2>
				<p>
					See the <a href="https://scholar.google.de/citations?user=upS2UTIAAAAJ" target="_blank">Google Scholar page of Prof. Klein</a> for a complete and up-to-date list, below some key publications are listed.
					For a complete list of publications, please refer to the following subpages:
					<ul>
						<li><a href="subpages_pubs/methodology.html">Peer-Reviewed Journals (Methodology)</a></li>
						<li><a href="subpages_pubs/applications.html">Peer-Reviewed Journals (Applications) </a></li>
						<li><a href="subpages_pubs/books.html">Books & Book Chapters</a></li>
						<li><a href="subpages_pubs/editorials.html">Editorials </a></li>
						<li><a href="subpages_pubs/conferences.html">Peer-Reviewed Conferences & Workshops </a></li>
						<li><a href="subpages_pubs/supplementary.html">Supplementary Materials</a></li>
						<li><a href="subpages_pubs/software.html">Software</a></li>
						<li><a href="subpages_pubs/working_papers.html">Working Papers </a></li>
					</ul>
				</p>

				<br>

				<h3>Selected Publications</h3>
				
					<ul class="fa-ul list-group" style="margin-left:25px">
						<li class="list-group-item">
							<div class="row">
								<div class="column2" style="width: 2%;">
									<span class="fa-li fa fa-check-square icon"></span>
								</div>

								<div class="column2" style="width: 50%;">
									<div class="content">
										<h4>Bayesian conditional transformation models <span class="label label-info"><a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.2023.2191820" target="_blank">pdf</a></span></h4>
										M. Carlan, T. Kneib and N. Klein<br>
										<i>To appear in Journal of the American Statistical Association</i>, 2023
									</div>
									<p style="text-align: justify"><u>Abstract:</u> Recent developments in statistical regression methodology shift away from pure mean regression toward distributional regression models. One important strand thereof is that of conditional transformation models (CTMs). CTMs infer the entire conditional distribution directly by applying a transformation function to the response conditionally on a set of covariates toward a simple log-concave reference distribution. Thereby, CTMs allow not only variance, kurtosis or skewness but the complete conditional distribution to depend on the explanatory variables. We propose a Bayesian notion of conditional transformation models (BCTMs) focusing on exactly observed continuous responses, but also incorporating extensions to randomly censored and discrete responses. Rather than relying on Bernstein polynomials that have been considered in likelihood-based CTMs, we implement a spline-based parameterization for monotonic effects that are supplemented with smoothness priors. Furthermore, we are able to benefit from the Bayesian paradigm via easily obtainable credible intervals and other quantities without relying on large sample approximations. A simulation study demonstrates the competitiveness of our approach against its likelihood-based counterpart but also Bayesian additive models of location, scale and shape and Bayesian quantile regression. Two applications illustrate the versatility of BCTMs in problems involving real world data, again including the comparison with various types of competitors. Supplementary materials for this article are available online.</p>
								</div>

								<div class="column2" style="width: 48%; margin-left: 10px; display: flex; align-items: center; justify-content: center;">
									<img src="images/key_pubs/BCTM.png" alt="BCTM" style="width: 100%; display: block; margin-left: auto; margin-right: auto;">
								</div>

							</div>
						</li>

						<br>

						<li class="list-group-item">
							<div class="row">
								<div class="column2" style="width: 2%;">
									<span class="fa-li fa fa-check-square icon"></span>
								</div>

								<div class="column2" style="width: 50%;">
									<div class="content">
										<h4>Deep Distributional Time Series Models and the Probabilistic Forecasting of Intraday Electricity Prices <span class="label label-info"><a href="https://onlinelibrary.wiley.com/doi/full/10.1002/jae.2959" target="_blank">pdf</a></span></h4>
										N. Klein, M. S. Smith and D. J. Nott<br>
										<i>Journal of Applied Econometrics, 38(4), 493-511</i>, 2023									
									</div>
									<p style="text-align: justify"><u>Abstract:</u> Recurrent neural networks (RNNs) with rich feature vectors of past values can provide accurate point forecasts for series that exhibit complex serial dependence. We propose two approaches to constructing deep time series probabilistic models based on a variant of RNN called an echo state network (ESN). The first is where the output layer of the ESN has stochastic disturbances and a Bayesian prior for regularization. The second employs the implicit copula of an ESN with Gaussian disturbances, which is a Gaussian copula process on the feature space. Combining this copula process with a nonparametrically estimated marginal distribution produces a distributional time series model. The resulting probabilistic forecasts are deep functions of the feature vector and marginally calibrated. In both approaches, Markov chain Monte Carlo methods are used to estimate the models and compute forecasts. The proposed models are suitable for the complex task of forecasting intraday electricity prices. Using data from the Australian market, we show that our deep time series models provide accurate short-term probabilistic price forecasts, with the copula model dominating. Moreover, the models provide a flexible framework for incorporating probabilistic forecasts of electricity demand, which increases upper tail forecast accuracy from the copula model significantly.</p>
								</div>

								<div class="column2" style="width: 48%; margin-left: 10px; display: flex; align-items: center; justify-content: center;">
									<img src="images/key_pubs/DeepDistributionalTimeSeries.png" alt="Deep Distributional Time Series Models and the Probabilistic Forecasting of Intraday Electricity Prices" style="width: 100%; display: block; margin-left: auto; margin-right: auto;">
								</div>

							</div>
						</li>	

						<br>

						<li class="list-group-item">
							<div class="row">
								<div class="column2" style="width: 2%;">
									<span class="fa-li fa fa-check-square icon"></span>
								</div>

								<div class="column2" style="width: 50%;">
									<div class="content">
										<h4>Semi-Structured Distributional Regression <span class="label label-info"><a href="https://www.tandfonline.com/doi/abs/10.1080/00031305.2022.2164054?journalCode=utas20" target="_blank">pdf</a></span></h4>
										D. Rügamer, C. Kolb and N. Klein<br>
										<i>To appear in The American Statistician</i>, 2023									
									</div>
									<p style="text-align: justify"><u>Abstract:</u> Combining additive models and neural networks allows to broaden the scope of statistical regression and extend deep learning-based approaches by interpretable structured additive predictors at the same time. Existing attempts uniting the two modeling approaches are, however, limited to very specific combinations and, more importantly, involve an identifiability issue. As a consequence, interpretability and stable estimation are typically lost. We propose a general framework to combine structured regression models and deep neural networks into a unifying network architecture. To overcome the inherent identifiability issues between different model parts, we construct an orthogonalization cell that projects the deep neural network into the orthogonal complement of the statistical model predictor. This enables proper estimation of structured model parts and thereby interpretability. We demonstrate the framework’s efficacy in numerical experiments and illustrate its special merits in benchmarks and real-world applications.</p>
								</div>

								<div class="column2" style="width: 48%; margin-left: 10px; display: flex; align-items: center; justify-content: center;">
									<img src="images/key_pubs/SemiStructuredDistributionalRegression.png" alt="Semi-Structured Distributional Regression" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
								</div>

							</div>
						</li>	

						<br>

						<li class="list-group-item">
							<div class="row">
								<div class="column2" style="width: 2%;">
									<span class="fa-li fa fa-check-square icon"></span>
								</div>

								<div class="column2" style="width: 50%;">
									<div class="content">
										<h4>Marginally calibrated response distributions for end-to-end learning in autonomous driving <span class="label label-info"><a href="https://projecteuclid.org/journals/annals-of-applied-statistics/volume-17/issue-2/Marginally-calibrated-response-distributions-for-end-to-end-learning-in/10.1214/22-AOAS1693.short" target="_blank">pdf</a></span></h4>
										C. Hoffmann and N. Klein<br>
										<i>Annals of Applied Statistics, 17(2), 1740-1763</i>, 2023
									</div>
									<p style="text-align: justify"><u>Abstract:</u> End-to-end learners for autonomous driving are deep neural networks that predict the instantaneous steering angle directly from images of the street ahead. These learners must provide reliable uncertainty estimates for their predictions in order to meet safety requirements and to initiate a switch to manual control in areas of high uncertainty. However, end-to-end learners typically only deliver point predictions, since distributional predictions are associated with large increases in training time or additional computational resources during prediction. To address this shortcoming, we investigate efficient and scalable approximate inference for the deep distributional model of Klein, Nott and Smith (J. Comput. Graph. Statist. 30 (2021) 467–483) in order to quantify uncertainty for the predictions of end-to-end learners. A special merit of this model, which we refer to as implicit copula neural linear model (IC-NLM), is that it produces densities for the steering angle that are marginally calibrated, that is, the average of the estimated densities equals the empirical distribution of steering angles. To ensure the scalability to large n regimes, we develop efficient estimation based on variational inference as a fast alternative to computationally intensive, exact inference via Hamiltonian Monte Carlo. We demonstrate the accuracy and speed of the variational approach on two end-to-end learners trained for highway driving using the comma2k19 dataset. The IC-NLM is competitive with other established uncertainty quantification methods for end-to-end learning in terms of nonprobabilistic predictive performance and outperforms them in terms of marginal calibration for in-distribution prediction. Our proposed approach also allows the identification of overconfident learners and contributes to the explainability of black-box end-to-end learners by using the predictive densities to understand which steering actions the learner sees as valid.</p>
								</div>

								<div class="column2" style="width: 48%; margin-left: 10px; display: flex; align-items: center; justify-content: center;">
									<img src="images/key_pubs/EndToEndAutonomousDriving.png" alt="Marginally calibrated response distributions for end-to-end learning in autonomous driving" style="width: 100%; display: block; margin-left: auto; margin-right: auto;">
								</div>

							</div>
						</li>	

						<br>

						<li class="list-group-item">
							<div class="row">
								<div class="column2" style="width: 2%;">
									<span class="fa-li fa fa-check-square icon"></span>
								</div>

								<div class="column2" style="width: 50%;">
									<div class="content">
										<h4>Boosting distributional copula regression <span class="label label-info"><a href="https://onlinelibrary.wiley.com/doi/full/10.1111/biom.13765" target="_blank">pdf</a></span></h4>
										N. Hans, N. Klein, F. Faschingbauer, M. Schneider and A. Mayr<br>
										<i>To appear in Biometrics,</i> 2022
									</div>
									<p style="text-align: justify"><u>Abstract:</u> Capturing complex dependence structures between outcome variables (e.g., study endpoints) is of high relevance in contemporary biomedical data problems and medical research. Distributional copula regression provides a flexible tool to model the joint distribution of multiple outcome variables by disentangling the marginal response distributions and their dependence structure. In a regression setup, each parameter of the copula model, that is, the marginal distribution parameters and the copula dependence parameters, can be related to covariates via structured additive predictors. We propose a framework to fit distributional copula regression via model-based boosting, which is a modern estimation technique that incorporates useful features like an intrinsic variable selection mechanism, parameter shrinkage and the capability to fit regression models in high-dimensional data setting, that is, situations with more covariates than observations. Thus, model-based boosting does not only complement existing Bayesian and maximum-likelihood based estimation frameworks for this model class but rather enables unique intrinsic mechanisms that can be helpful in many applied problems. The performance of our boosting algorithm for copula regression models with continuous margins is evaluated in simulation studies that cover low- and high-dimensional data settings and situations with and without dependence between the responses. Moreover, distributional copula boosting is used to jointly analyze and predict the length and the weight of newborns conditional on sonographic measurements of the fetus before delivery together with other clinical variables.</p>
								</div>

								<div class="column2" style="width: 48%; margin-left: 10px; display: flex; align-items: center; justify-content: center;">
									<img src="images/key_pubs/BoostingDistrCopula.png" alt="Boosting distributional copula regression" style="width: 100%; display: block; margin-left: auto; margin-right: auto;">
								</div>

							</div>
						</li>	

						<br>
						
						<li class="list-group-item">
							<div class="row">
								<div class="column2" style="width: 2%;">
									<span class="fa-li fa fa-check-square icon"></span>
								</div>

								<div class="column2" style="width: 50%;">
									<div class="content">
										<h4>Distributional Adaptive Soft Regression Trees <span class="label label-info"><a href="https://arxiv.org/abs/2210.10389" target="_blank">pdf</a></span></h4>
										N. Umlauf and N. Klein, 2022<br>
										<i>Submitted.</i>
									</div>
									<p style="text-align: justify"><u>Abstract:</u> Random forests are an ensemble method relevant for many problems, such as regression or classification. They are popular due to their good predictive performance (compared to, e.g., decision trees) requiring only minimal tuning of hyperparameters. They are built via aggregation of multiple regression trees during training and are usually calculated recursively using hard splitting rules. Recently regression forests have been incorporated into the framework of distributional regression, a nowadays popular regression approach aiming at estimating complete conditional distributions rather than relating the mean of an output variable to input features only - as done classically. This article proposes a new type of a distributional regression tree using a multivariate soft split rule. One great advantage of the soft split is that smooth high-dimensional functions can be estimated with only one tree while the complexity of the function is controlled adaptive by information criteria. Moreover, the search for the optimal split variable is obsolete. We show by means of extensive simulation studies that the algorithm has excellent properties and outperforms various benchmark methods, especially in the presence of complex non-linear feature interactions. Finally, we illustrate the usefulness of our approach with an example on probabilistic forecasts for the Sun's activity.</p>
								</div>

								<div class="column2" style="width: 48%; margin-left: 10px; display: flex; align-items: center; justify-content: center;">
									<img src="images/key_pubs/DistrAdaptiveTrees.png" alt="Distributional Adaptive Soft Regression Trees" style="width: 90%; display: block; margin-left: auto; margin-right: auto;">
								</div>

							</div>
						</li>

						<br>

						<li class="list-group-item">
							<div class="row">
								<div class="column2" style="width: 2%;">
									<span class="fa-li fa fa-check-square icon"></span>
								</div>

								<div class="column2" style="width: 50%;">
									<div class="content">
										<h4>Marginally Calibrated Deep Distributional Regression <span class="label label-info"><a href="https://arxiv.org/abs/1908.09482" target="_blank">pdf</a></span></h4>
										N. Klein, D. J. Nott and M. S. Smith<br>
										<i>Journal of Computational and Graphical Statistics, 30(2), 467-483</i>, 2021
									</div>
									<p style="text-align: justify"><u>Abstract:</u> Deep neural network (DNN) regression models are widely used in applications requiring state-of-the-art predictive accuracy. However, until recently there has been little work on accurate uncertainty quantification for predictions from such models. We add to this literature by outlining an approach to constructing predictive distributions that are `marginally calibrated'. This is where the long run average of the predictive distributions of the response variable matches the observed empirical margin. Our approach considers a DNN regression with a conditionally Gaussian prior for the final layer weights, from which an implicit copula process on the feature space is extracted. This copula process is combined with a non-parametrically estimated marginal distribution for the response. The end result is a scalable distributional DNN regression method with marginally calibrated predictions, and our work complements existing methods for probability calibration. The approach is first illustrated using two applications of dense layer feed-forward neural networks. However, our main motivating applications are in likelihood-free inference, where distributional deep regression is used to estimate marginal posterior distributions. In two complex ecological time series examples we employ the implicit copulas of convolutional networks, and show that marginal calibration results in improved uncertainty quantification. Our approach also avoids the need for manual specification of summary statistics, a requirement that is burdensome for users and typical of competing likelihood-free inference methods.</p>
								</div>

								<div class="column2" style="width: 48%; margin-left: 10px; display: flex; align-items: center; justify-content: center;">
									<img src="images/key_pubs/MargCalibDistrRegression.png" alt="Distributional Adaptive Soft Regression Trees" style="width: 100%; display: block; margin-left: auto; margin-right: auto;">
								</div>

							</div>
						</li>	

						<br>

						<li class="list-group-item">
							<div class="row">
								<div class="column2" style="width: 2%;">
									<span class="fa-li fa fa-check-square icon"></span>
								</div>

								<div class="column2" style="width: 50%;">
									<div class="content">
										<h4>Bayesian Inference for Regression Copulas <span class="label label-info"><a href="https://arxiv.org/abs/1907.04529" target="_blank">pdf</a></span></h4>
										M. S. Smith and N. Klein<br>
										<i>Journal of Business and Economic Statistics, 39(3), 712-728</i>, 2021
									</div>
									<p style="text-align: justify"><u>Abstract:</u> We propose a new semi-parametric distributional regression smoother that is based on a copula decomposition of the joint distribution of the vector of response values. The copula is high-dimensional and constructed by inversion of a pseudo regression, where the conditional mean and variance are semi-parametric functions of covariates modeled using regularized basis functions. By integrating out the basis coefficients, an implicit copula process on the covariate space is obtained, which we call a `regression copula'. We combine this with a non-parametric margin to define a copula model, where the entire distribution - including the mean and variance - of the response is a smooth semi-parametric function of the covariates. The copula is estimated using both Hamiltonian Monte Carlo and variational Bayes; the latter of which is scalable to high dimensions. Using real data examples and a simulation study we illustrate the efficacy of these estimators and the copula model. In a substantive example, we estimate the distribution of half-hourly electricity spot prices as a function of demand and two time covariates using radial bases and horseshoe regularization. The copula model produces distributional estimates that are locally adaptive with respect to the covariates, and predictions that are more accurate than those from benchmark models.</p>
								</div>

								<div class="column2" style="width: 48%; margin-left: 10px; display: flex; align-items: center; justify-content: center;">
									<img src="images/key_pubs/BayesInfRegressionCopulas.png" alt="Bayesian Inference for Regression Copulas" style="width: 100%; display: block; margin-left: auto; margin-right: auto;">
								</div>

							</div>
						</li>
						
						<br>

						<li class="list-group-item">
							<div class="row">
								<div class="column2" style="width: 2%;">
									<span class="fa-li fa fa-check-square icon"></span>
								</div>

								<div class="column2" style="width: 50%;">
									<div class="content">
										<h4>Modelling Regional Patterns of Inefficiency: A Bayesian Approach to Geoadditive Panel Stochastic Frontier Analysis with an Application to Cereal Production in England and Wales <span class="label label-info"><a href="https://www.sciencedirect.com/science/article/abs/pii/S0304407619301587" target="_blank">pdf</a></span></h4>
										N. Klein, H. Herwartz and T. Kneib<br>
										<i>Journal of Econometrics, 214(2), 513-539</i>, 2020
									</div>
									<p style="text-align: justify"><u>Abstract:</u> We propose a flexible Bayesian approach to inefficiency modelling that accounts for regional patterns of local performance. The model allows for a separated treatment of individual heterogeneity and determinants of inefficiency. Regional dependence structures and location-specific unobserved spatial heterogeneity are modelled via geoadditive predictors in the inefficiency term of the stochastic frontier model. Inference becomes feasible through Markov chain Monte Carlo simulation techniques. In an empirical illustration we find that regional patterns of inefficiency characterize cereal production in England and Wales. Neglecting common performance patterns of farms located in the same region induces systematic biases to inefficiency estimates.</p>
								</div>

								<div class="column2" style="width: 48%; margin-left: 10px; display: flex; align-items: center; justify-content: center;">
									<img src="images/key_pubs/GeoadditivePanelAnalysis.png" alt="Modelling Regional Patterns of Inefficiency: A Bayesian Approach to Geoadditive Panel Stochastic Frontier Analysis with an Application to Cereal Production in England and Wales" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
								</div>

							</div>
						</li>

						<br>

						<li class="list-group-item">
							<div class="row">
								<div class="column2" style="width: 2%;">
									<span class="fa-li fa fa-check-square icon"></span>
								</div>

								<div class="column2" style="width: 50%;">
									<div class="content">
										<h4>Bayesian Generalized Additive Models for Location, Scale and Shape for Zero-Inflated and Overdispersed Count Data <span class="label label-info"><a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.2014.912955" target="_blank">pdf</a></span></h4>
										N. Klein, T. Kneib and S. Lang<br>
										<i>Journal of the American Statistical Association, 110(509), 405-419</i>, 2015
									</div>
									<p style="text-align: justify"><u>Abstract:</u> Frequent problems in applied research preventing the application of the classical Poisson log-linear model for analyzing count data include overdispersion, an excess of zeros compared to the Poisson distribution, correlated responses, as well as complex predictor structures comprising nonlinear effects of continuous covariates, interactions or spatial effects. We propose a general class of Bayesian generalized additive models for zero-inflated and overdispersed count data within the framework of generalized additive models for location, scale, and shape where semiparametric predictors can be specified for several parameters of a count data distribution. As standard options for applied work we consider the zero-inflated Poisson, the negative binomial and the zero-inflated negative binomial distribution. The additive predictor specifications rely on basis function approximations for the different types of effects in combination with Gaussian smoothness priors. We develop Bayesian inference based on Markov chain Monte Carlo simulation techniques where suitable proposal densities are constructed based on iteratively weighted least squares approximations to the full conditionals. To ensure practicability of the inference, we consider theoretical properties like the involved question whether the joint posterior is proper. The proposed approach is evaluated in simulation studies and applied to count data arising from patent citations and claim frequencies in car insurances. For the comparison of models with respect to the distribution, we consider quantile residuals as an effective graphical device and scoring rules that allow us to quantify the predictive ability of the models. The deviance information criterion is used to select appropriate predictor specifications once a response distribution has been chosen. Supplementary materials for this article are available online.</p>
								</div>

								<div class="column2" style="width: 48%; margin-left: 10px; display: flex; align-items: center; justify-content: center;">
									<img src="images/key_pubs/BayesAdditiveScaleShape.png" alt="Bayesian Generalized Additive Models for Location, Scale and Shape for Zero-Inflated and Overdispersed Count Data" style="width: 100%; display: block; margin-left: auto; margin-right: auto;">
								</div>

							</div>
						</li>	

					</ul>
			</div>
		</section>		

		
			


			<!-- Imprint and data protection -->
			<div style="display: inline-block; text-align: right; width: 100%; margin-top: 15px;">
				<a href="https://www.uaruhr.de/impressum/index.html.en">Imprint</a> / <a href="https://www.uaruhr.de/impressum/datenschutz.html.en">Data Protection</a>
			</div>
			
		</div>

				
	</div>
		
	
  </div>
</div>

	<!-- jquery -->
	<script src="./libs/jquery-3.6.1.min.js"></script>
	
	<!-- boostrap -->
	<script src="./libs/bootstrap-3.4.1/js/bootstrap.min.js"></script>

	<!-- Functionality for popover of buttons -->
	<script>
		$(document).ready(function(){
			$('[data-toggle="popover"]').popover(); 
		});
	</script>

	<!-- Functionality for scroll-up button -->
	<script>
	$(document).ready(function(){ 
		if ($(this).scrollTop() > 100) {
			$('#scrollbutton').fadeIn();
		}
		$(window).scroll(function(){ 
			if ($(this).scrollTop() > 100) { 
				$('#scrollbutton').fadeIn();
			} else { 
				$('#scrollbutton').fadeOut(); 
			}
		});
		$('#scrollbutton').click(function(){ 
			$("html, body").animate({ scrollTop: 0 }, 300); 
			return false; 
		}); 
	});
	</script>
	
</body>

</html>
